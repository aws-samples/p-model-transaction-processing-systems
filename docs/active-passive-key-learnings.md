# Active-Passive - Key Learnings

1. **Replication Strategy has Implications on potential Data Loss**: In order to implement an active-passive architecture, customers generally use a database, such as Aurora Global Database, with an active database in active region and passive database in passive region and design the application to write to active database, and use a replicator to replicate data from active database to the passive database in an asynchronous manner. This architecture is vulnerable to potential data loss, if either the active database or replicator is impaired before all the records written to active database is replicated to passive database. A) *RPO* *Monitoring*: Customers need to monitor the replication lag to ensure that the replication lag is below acceptable RPO. B) *RPO Impact*: During failover process, customers need to ensure that all records are replicated before switching active and passive database to achieve zero RPO, or switch the active and passive database without waiting for replication to complete and be prepared to handle data loss resulting from records pending replication that may result in non-zero RPO C) *Temporary Dataloss*: If the failure in active database and/or replicator is temporary in nature and does not involve irrevocable permanent data loss owing to large scale natural or man-made disaster, then it is possible to recover from temporary data loss without permanent loss of data. P model was very useful in describing an Aurora Global Database, as a pair of Aurora Regional Database in two regions, with a Aurora Replicator replicating data from active database to passive database. We were also able to simulate Aurora Database Failover from active region to passive region by describing region1-active-region2-passive and region1-passive-region2-active as two different states, both in the Aurora Global Database (to direct traffic to active database) and also Aurora Replicator (to change the direction of replication). We were also able to simulate various failure scenario, such as failing the Aurora Global Database or failing only Aurora Replicator, to evaluate and validate the system invariants under such failure scenarios. In another P Model simulation, we will explore the data loss implications of synchronous database writes to multiple regions instead of relying on asynchronous replication..
2. **Applications need to be Designed to handle Dependency Failure Elegantly**: In this architecture, applications logic is implemented as stateless containers that A) read a record from an input queue, B) write the record to a database C) write the record to output queue and D) then remove the record from the input queue. This is a typical implementation of an application that ensures transaction integrity. If a step in the process fails, then the subsequent steps are not executed. For example, if step B fails when trying to write a record to the database, then step C, writing to output queue or step D, removing from input queue is not executed. Also, if step B, writing to database succeeds, but, step C, writing to outbound queue fails, then also step D, removing from input queue is not executed. P Model is useful in describing these invariants. 1) If step D, removing from input queue is executed when step B, writing to database fails, then it will translate to “correctness” failure. The P model assumes that an application is considered to be “correct”, if it removes a record from the input queue only after the record is written to the database, ensuring that all records published by the source are persisted in the databases. 2) If step D, removing from input queue is executed when step C, writing to output queue fails, then it will translate to “liveness” failure. The P model assumes that an application is considered live, if all the records published by source is delivered to all the components in the architecture and finally to the destination. 3) The P model also implements an additional “correctness” requirement, that requires all the records to be delivered to all the components in the architecture and finally to the destination “in the order” they were published by the source. Also, there is a the scenario of incomplete transaction that may lead to an inconsistent state. If step B, writing to database succeeds, but step C, writing to output queue fails, then step D, removing from input queue is not executed. This leads to the state that the record is still in the input queue, even though it is written to the database. This can be handled in one of three ways. 1) Execute all the 3 steps, B, C and D, using an external transaction manager, which ensures all of them are executed or none of them are executed if any of them records a failure. 2) Implement a compensating mechanism, that removes the record from the database, step B, if step C, writing to output queue fails. 3) Implement each component as idempotent, that can process the same record multiple times, without leading to duplicate processing, by processing the record only the first time and skip processing the record when it is received more than once. The P model has adopted the approach #3, by processing a record once and ignoring subsequent processing.
3. **Handle Internal Failure and Dependency Failure Differently**: We have used stateless containers to model the application. It is important to differentiate application failure caused by internal application processing (such as application failure due to bad data or application failure due to host failure) and application failure caused by dependency failure (such as failure in an infrastructure component such as queue or database). If a container goes down due to internal application failure, then launching another container is likely to continue processing, where the last container left off. This process can be automated with auto-scaling. However, with dependency failure, the dependency needs to be recovered before the container can be re-started, otherwise, based on the type of dependency failure, the restarted container may not perform as expected or may go down again with an error message leading to continuous cycle of container starting up and going down. We were able to use P to model internal failures, by failing the containers and model external failures, by failing the queues and databases. Initially, we implemented the P model for containers by approaching all kinds of failures in a uniform way. Then we realized, these two kinds of failures required separate treatments and implemented 3 states in the containers: 1) process records 2) failed processing 3) failed dependency. Anytime, the container in “process records” encountered a failure in input queue, database or output queue, it entered “failed dependency” state. When, we recovered the failed dependencies and recovered the container, it established new connection with the dependencies and continued processing. Anytime, the container encountered internal application error, when such an internal failure is injected through an event, it entered “failed processing”. Recovering from this state was straight forward.
4. **Process Records in Order by Leveraging Appropriate Infrastructure**: In certain business applications, it is important to process records in order. We leveraged appropriate infrastructure such as FIFO Queue and Aurora Global Database that are designed to maintain order of processing. Assuming that underlying infrastructure maintains order of processing, the P Model was able to demonstrate that the records were delivered to each component and finally to the record destination in order, by implementing this as invariant in the Active-Passive Specification.
5. **Replay not required if Infrastructure Components do not lose Data**: If infrastructure components, such as queues and databases, do not lose data under failure scenarios, then all messages published by the source are delivered to all the components in the system and destination, while preserving order of processing, even though the queues and databases in the architecture were failed at random, without requiring that record source replay messages.  We demonstrated this by recovering impacted containers from failures of queues and databases, by simulating the failures through P models and implementing the invariants in the Active-Passive Specification.
6. **Leverage Failover DNS Routing for Active-Passive Failover**: In order to support active-passive failover, we created a P model for DNS routing that uses failover routing policy. This allows messages published by record source to be routed to the inbound queue of the active region, depending on which region is active. Failover involving failing over the DNS routing so that all traffic is failed over from region 1 to region 2 and also failing over the database, so all database transactions  are applied to region 2, and replicated to region 1.
7. **Simulating Record Source and Record Destination allowed the Application to be treated as a Back Box**: We created a P model for record source that can publish messages to a DNS target and created a P model for record destination that can receive messages from a DNS target. This allowed us to treat the entire application as a black box and validate the invariants at source and destination level.
