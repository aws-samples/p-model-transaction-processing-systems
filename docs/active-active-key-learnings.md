# Active-Active - Key Learnings

1. **Replication Strategy has Implications on potential Data Loss**: In order to implement an active-active architecture, customers generally use a database, such as DynamoDB Global Tables, with an active database in each region and design the application to write to the active databases in respective regions, and use a replicator to replicate data from one region to another region in an asynchronous manner. This architecture is vulnerable to potential data loss, if either of the active databases or replicator is impaired before all the records written to an active database in one region is replicated to the other database. A) *RPO* *Monitoring*: Customers need to monitor the replication lag to ensure that the replication lag is below acceptable RPO. B) *RPO Impact*: Failover in an active-active database involves taking the active database in the impacted region offline and continue processing with the other database. During the failover process, customers need to ensure that all records are replicated from an active database in the impacted region to the active database in the other region, before taking the active database in the impacted region offline to achieve zero RPO, or take the active database in the impacted region offline waiting for replication to complete and be prepared to handle data loss resulting from records pending replication that may result in non-zero RPO C) *Temporary Dataloss*: If the failure in an active database and/or replicator is temporary in nature and does not involve irrevocable permanent data loss owing to large scale natural or man-made disaster, then it is possible to recover from temporary data loss without permanent loss of data. P model was very useful in describing a DynamoDB Global Table, as a pair of DynamoDB Regional Tables in two regions, with a DynamoDB Replicator replicating data from one region to another region and vice versa. We take an active database offline by using weighted DNS routing and changing the weight of impacted region to zero, so that all read and write traffic is directed to the other region. We were also able to simulate various failure scenario, such as failing the DynamoDB Global Table or failing only DynamoDB Replicator, to evaluate and validate the system invariants under such failure scenarios. In the third workload mentioned below, we will explore the data loss implications of synchronous (or blocking) database writes to multiple regions instead of relying on asynchronous replication.
2. **Applications need to be Designed to handle Dependency Failure Elegantly**: In this architecture, applications logic is implemented as stateless containers that A) read a record from an input queue, B) write the record to a database C) write the record to output queue and D) then remove the record from the input queue. This is a typical implementation of an application that ensures transaction integrity. If a step in the process fails, then the subsequent steps are not executed. For example, if step B fails when trying to write a record to the database, then step C, writing to output queue or step D, removing from input queue is not executed. Also, if step B, writing to database succeeds, but, step C, writing to outbound queue fails, then also step D, removing from input queue is not executed. P Model is useful in describing these invariants. 1) If step D, removing from input queue is executed when step B, writing to database fails, then it will translate to “correctness” failure. The P model assumes that an application is considered to be “correct”, if it removes a record from the input queue only after the record is written to the database, ensuring that all records published by the source are persisted in the databases. 2) If step D, removing from input queue is executed when step C, writing to output queue fails, then it will translate to “liveness” failure. The P model assumes that an application is considered live, if all the records published by source is delivered to all the components in the architecture and finally to the destination. 3) The P model also implements an additional “correctness” requirement, that requires all the records to be delivered to all the components in the architecture and finally to the destination “in the order” they were published by the source. Also, there is a the scenario of incomplete transaction that may lead to an inconsistent state. If step B, writing to database succeeds, but step C, writing to output queue fails, then step D, removing from input queue is not executed. This leads to the state that the record is still in the input queue, even though it is written to the database. This can be handled in one of three ways. 1) Execute all the 3 steps, B, C and D, using an external transaction manager, which ensures all of them are executed or none of them are executed if any of them records a failure. 2) Implement a compensating mechanism, that removes the record from the database, step B, if step C, writing to output queue fails. 3) Implement each component as idempotent, that can process the same record multiple times, without leading to duplicate processing, by processing the record only the first time and skip processing the record when it is received more than once. The P model has adopted the approach #3, by processing a record once and ignoring subsequent processing.
3. **Handle Internal Failure and Dependency Failure Differently**: We have used stateless containers to model the application. It is important to differentiate application failure caused by internal application processing (such as application failure due to bad data or application failure due to host failure) and application failure caused by dependency failure (such as failure in an infrastructure component such as queue or database). If a container goes down due to internal application failure, then launching another container is likely to continue processing, where the last container left off. This process can be automated with auto-scaling. However, with dependency failure, the dependency needs to be recovered before the container can be re-started, otherwise, based on the type of dependency failure, the restarted container may not perform as expected or may go down again with an error message leading to continuous cycle of container starting up and going down. We were able to use P to model internal failures, by failing the containers and model external failures, by failing the queues and databases. Initially, we implemented the P model for containers by approaching all kinds of failures in a uniform way. Then we realized, these two kinds of failures required separate treatments and implemented 3 states in the containers: 1) process records 2) failed processing 3) failed dependency. Anytime, the container in “process records” encountered a failure in input queue, database or output queue, it entered “failed dependency” state. When, we recovered the failed dependencies and recovered the container, it established new connection with the dependencies and continued processing. Anytime, the container encountered internal application error, when such an internal failure is injected through an event, it entered “failed processing”. Recovering from this state was straight forward.
4. **Ordering is Difficult to Enforce in an Active-Active Infrastructure**: In an active-active architecture half the records published by record source are routed to region 1 and half the records are routed to region 2, as the architecture uses DNS routing with weighted routing policy with equal weights assigned to both the regions. In such a scenario, the records sent to both the regions follow independent routes to reach the record destination. Consequently, we can’t expect that the records will be delivered to the record destination in the same order as it was published by record source. When we created state machines for all the components using P and published messages to both the regions which were processed independently and asynchronously by the state machines, we noticed that “ordering requirement” initially specified in the active-active specification, failed consistently. So, we retained the “correctness” requirement that all the records are delivered to all the components and the record destination, and removed the “correctness” requirement that they need to be delivered “in order”.
5. **Replay not required if Infrastructure Components do not lose Data**: If infrastructure components, such as queues and databases, do not lose data under failure scenarios, then all messages published by the source are delivered to all the components in the system and destination, even though the queues and databases in the architecture were failed at random, without requiring that record source replay messages.  We demonstrated this by recovering impacted containers from failures of queues and databases, by simulating the failures through P models and implementing the invariants in the Active-Active Specification.
6. **Leverage Weighted DNS Routing for Active-Active Failover**: In order to support active-active failover, we created a P model for DNS routing that uses weighted routing policy and assigning equal weights to the two regions. This allows records published by record source to be routed to the inbound queues both regions in equal distribution. Failover involving updating the weight for a region to zero, thus routing all traffic to the other region.
7. **Simulating Record Source and Record Destination allowed the Application to be treated as a Back Box**: We created a P model for record source that can publish messages to a DNS target and created a P model for record destination that can receive messages from a DNS target. This allowed us to treat the entire application as a black box and validate the invariants at source and destination level.