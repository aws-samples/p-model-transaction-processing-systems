# Workload1 - Key Learnings

1. **Replication Strategy has Implications on potential Data Loss**: In order to implement an active-active architecture, customers have started using databases, such as MongoDB Atlas, that support stronger consistency model (e.g. acknowledge commit only after replicated to majority of nodes: https://www.youtube.com/watch?v=CV1i5d5qs5w). This architecture protects from potential data loss, if the master databases is impaired. In a 3 node cluster, the primary replicates to one of the secondary nodes, before acknowledging a transaction. In a 5 node cluster, the primary replicates to two of the secondary nodes, before acknowledging a transaction. If the primary node is not available one of the secondary nodes takes over as primary. The architecture protects against degradation in one of the regions. It is still vulnerable to potential data loss if multiple regions experience degradation simultaneously. P model was very useful in describing a MongoDB Atlas Database, as a triad of MongoDB Regional Database in three regions, with region 1 designated as primary, region 2 as secondary and region 3 as tertiary. When region 1 is active, a record inserted into MongoDB is first inserted into region 1 database, then replicated to region 2 database, once region 2 replication is complete, region 3 is replication is initiated asynchronously and the transaction is acknowledged as successful. When region 2 is active, a record inserted into MongoDB is first inserted into region 2 database, then replicated to region 3 database, once region 3 replication is complete, region 1 replication is initiated asynchronously and transaction is acknowledged as successful. We take an active database offline by using weighted DNS routing and changing the weight of impacted region to zero, so that all read and write traffic is directed to the other region and also instructing the database to switch regions. We were also able to simulate various failure scenario, such as failing the MongoDB Atlas, to evaluate and validate the system invariants under such failure scenarios.
2. **Applications need to be Designed to handle Dependency Failure Elegantly**: In this architecture, applications logic is implemented as stateless containers. Let us analyze the processor container in detail. It A) reads records from an input Kafka stream, B) write the record to a database C) write the record to output Kafka stream and D) then remove the record from the input Kafka stream. This is a typical implementation of an application that ensures transaction integrity. If a step in the process fails, then the subsequent steps are not executed. For example, if step B fails when trying to write a record to the database, then step C, writing to output Kafka stream or step D, removing from input Kafka is not executed. Also, if step B, writing to database succeeds, but, step C, writing to outbound Kafka stream fails, then also step D, removing from input Kafka stream is not executed. P Model is useful in describing these invariants. 1) If step D, removing from input Kafka stream is executed when step B, writing to database fails, then it will translate to “correctness” failure. The P model assumes that an application is considered to be “correct”, if it removes a record from the input Kafka stream only after the record is written to the database, ensuring that all records published by the source are persisted in the databases. 2) If step D, removing from input Kafka stream is executed when step C, writing to output Kafka stream fails, then it will translate to “liveness” failure. The P model assumes that an application is considered live, if all the records published by source is delivered to all the components in the architecture and finally to the destination. Also, there is a the scenario of incomplete transaction that may lead to an inconsistent state. If step B, writing to database succeeds, but step C, writing to output Kafka stream fails, then step D, removing from input Kafka stream is not executed. This leads to the state that the record is still in the input Kafka stream, even though it is written to the database. This can be handled in one of three ways. 1) Execute all the 3 steps, B, C and D, using an external transaction manager, which ensures all of them are executed or none of them are executed if any of them records a failure. 2) Implement a compensating mechanism, that removes the record from the database, step B, if step C, writing to output Kafka stream fails. 3) Implement each component as idempotent, that can process the same record multiple times, without leading to duplicate processing, by processing the record only the first time and skip processing the record when it is received more than once. The P model has adopted the approach #3, by processing a record once and ignoring subsequent processing.
3. **Handle Internal Failure and Dependency Failure Differently**: We have used stateless containers to model the application. It is important to differentiate application failure caused by internal application processing (such as application failure due to bad data or application failure due to host failure) and application failure caused by dependency failure (such as failure in an infrastructure component such as Kafka stream or database). If a container goes down due to internal application failure, then launching another container is likely to continue processing, where the last container left off. This process can be automated with auto-scaling. However, with dependency failure, the dependency needs to be recovered before the container can be re-started, otherwise, based on the type of dependency failure, the restarted container may not perform as expected or may go down again with an error message leading to continuous cycle of container starting up and going down. We were able to use P to model internal failures, by failing the containers and model external failures, by failing the Kafka stream and databases. Initially, we implemented the P model for containers by approaching all kinds of failures in a uniform way. Then we realized, these two kinds of failures required separate treatments and implemented 3 states in the containers: 1) process records 2) failed processing 3) failed dependency. Anytime, the container in “process records” encountered a failure in input Kafka stream, database or output Kafka stream, it entered “failed dependency” state. When, we recovered the failed dependencies and recovered the container, it established new connection with the dependencies and continued processing. Anytime, the container encountered internal application error, when such an internal failure is injected through an event, it entered “failed processing”. Recovering from this state was straight forward.
4. **Ordering is Difficult to Enforce in an Active-Active Infrastructure**: In an active-active architecture half the records published by record source are routed to region 1 and half the records are routed to region 2, as the architecture uses DNS routing with weighted routing policy with equal weights assigned to both the regions. In such a scenario, the records sent to both the regions follow independent routes to reach the record destination. Consequently, we can’t expect that the records will be delivered to the record destination in the same order as it was published by record source. When we created state machines for all the components using P and published messages to both the regions which were processed independently and asynchronously by the state machines, we noticed that “ordering requirement” initially specified in the active-active specification, failed consistently. So, we retained the “correctness” requirement that all the records are delivered to all the components and the record destination, and removed the “correctness” requirement that they need to be delivered “in order”.
5. **Make Components Idempotent to handle Duplicate Records**: When we were simulating different types of failure scenarios using P, we noticed that under certain failure scenarios, in the presence of certain race conditions (an input Kafka stream was failed before removal of a record from the input Kafka stream could be confirmed resulting in Kafka stream delivering the record twice), some edge records were delivered twice. We made the containers idempotent to account for such duplicate delivery of edge records and incorporated in the Active-Active Specification.
6. **Replay not required if Infrastructure Components do not lose Data**: If infrastructure components, such as Kafka streams and databases, do not lose data under failure scenarios, then all messages published by the source are delivered to all the components in the system and destination, even though the Kafka streams and databases in the architecture were failed at random, without requiring that record source replay messages.  We demonstrated this by recovering impacted containers from failures of Kafka streams and databases, by simulating the failures through P models and implementing the invariants in the Active-Active Specification.
7. **Leverage Weighted DNS Routing for Active-Active Failover**: In order to support active-active failover, we created a P model for DNS routing that uses weighted routing policy and assigning equal weights to the two regions. This allows records published by record source to be routed to the inbound Kafka streams both regions in equal distribution. Failover involving updating the weight for a region to zero, thus routing all traffic to the other region.
8. **Simulating Record Source and Record Destination allowed the Application to be treated as a Back Box**: We created a P model for record source that can publish messages to a DNS target and created a P model for record destination that can receive messages from a DNS target. This allowed us to treat the entire application as a black box and validate the invariants at source and destination level.